{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQRh9TDkmexb"
   },
   "source": [
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onZXfhnkmexc"
   },
   "source": [
    "# Introduction\n",
    "In this notebook we demonstrates how to train forecaster on one node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we need to train a forecaster? \n",
    "To learn the pattern (like the period, scale...) in history data.\n",
    "\n",
    "### Hardware support\n",
    "Chronos only support CPU training (and acceleration on Intel CPU) for training. Since time series data is not large, CPU training is enough for most cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8kjsyL4mexd"
   },
   "source": [
    "# Step1: Data preparation\n",
    "We support 3 data representations:\n",
    "\n",
    "### TSDataset\n",
    "\n",
    "Forecaster will automatically process the TSDataset. By default, TSDataset will be transformed to a pytorch dataloader, which is memory-friendly while a little bit slower.  \n",
    "Users may call `roll` on the TSDataset before calling `fit`. Then the training speed will be faster but will consume more memory.  \n",
    "\n",
    "Below cell shows how to generate the TSDataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NC4pcIizmexe",
    "outputId": "d266f123-ba84-422c-82c3-cb12c2768776"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def create_tsdataset(roll=True, horizon=5):\n",
    "    from bigdl.chronos.data import TSDataset\n",
    "    import pandas as pd\n",
    "    timeseries = pd.date_range(start='2020-01-01', freq='D', periods=1000)\n",
    "    df = pd.DataFrame(np.random.rand(1000, 2),\n",
    "                      columns=['value1', 'value2'],\n",
    "                      index=timeseries)\n",
    "    df.reset_index(inplace=True)\n",
    "    df.rename(columns={'index': 'timeseries'}, inplace=True)\n",
    "    train, _, test = TSDataset.from_pandas(df=df,\n",
    "                                           dt_col='timeseries',\n",
    "                                           target_col=['value1', 'value2'],\n",
    "                                           with_split=True)\n",
    "    if roll:\n",
    "        for tsdata in [train, test]:\n",
    "            tsdata.roll(lookback=24, horizon=horizon)\n",
    "    return train, test\n",
    "train, test = create_tsdataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numpy Ndarray\n",
    "\n",
    "x's shape is (num_samples, lookback, feature_dim) where lookback and feature_dim should be the same as past_seq_len and input_feature_num.  \n",
    "y's shape is (num_samples, horizon, target_dim), where horizon and target_dim should be the same as future_seq_len and output_feature_num."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two cells show how to generate the Numpy Ndarray using either pytorch or tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y8mUaHScU61U",
    "outputId": "5cb15a29-a5e4-4ffd-90ac-50ff8427ab87"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "def create_data(loader=False):\n",
    "    num_train_samples = 1000\n",
    "    num_val_samples = 400\n",
    "    num_test_samples = 400\n",
    "    input_time_steps = 24\n",
    "    input_feature_dim = 1\n",
    "    output_time_steps = 5\n",
    "    output_feature_dim = 1\n",
    "\n",
    "    def get_x_y(num_samples):\n",
    "        x = np.random.rand(num_samples, input_time_steps, input_feature_dim).astype(np.float32)\n",
    "        y = x[:, -output_time_steps:, :]*2 + \\\n",
    "            np.random.rand(num_samples, output_time_steps, output_feature_dim).astype(np.float32)\n",
    "        return x, y\n",
    "\n",
    "    train_data = get_x_y(num_train_samples)\n",
    "    val_data = get_x_y(num_val_samples)\n",
    "    test_data = get_x_y(num_test_samples)\n",
    "\n",
    "    if loader:\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "        train_loader = DataLoader(TensorDataset(torch.from_numpy(train_data[0]),\n",
    "                                                torch.from_numpy(train_data[1])), batch_size=32)\n",
    "        val_loader = DataLoader(TensorDataset(torch.from_numpy(val_data[0]),\n",
    "                                              torch.from_numpy(val_data[1])), batch_size=32)\n",
    "        test_loader = DataLoader(TensorDataset(torch.from_numpy(test_data[0]),\n",
    "                                               torch.from_numpy(test_data[1])), batch_size=32)\n",
    "        return train_loader, val_loader, test_loader\n",
    "    else:\n",
    "        return train_data, val_data, test_data\n",
    "train_data, val_data, test_data = create_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "def create_data(tf_data=False, batch_size=32):\n",
    "    train_num_samples = 1000\n",
    "    test_num_samples = 400\n",
    "    input_feature_num = 10\n",
    "    output_feature_num = 2\n",
    "    past_seq_len = 10\n",
    "    \n",
    "    def get_x_y(num_sample):\n",
    "        x = np.random.randn(num_sample, past_seq_len, input_feature_num)\n",
    "        y = np.random.randn(num_sample, 1, output_feature_num)\n",
    "        return x, y\n",
    "    \n",
    "    train_data = get_x_y(train_num_samples)\n",
    "    test_data = get_x_y(test_num_samples)\n",
    "\n",
    "    if tf_data:\n",
    "        from_tensor_slices = tf.data.Dataset.from_tensor_slices\n",
    "        train_data = from_tensor_slices(train_data).cache()\\\n",
    "                                                   .shuffle(train_num_samples)\\\n",
    "                                                   .batch(batch_size)\\\n",
    "                                                   .prefetch(tf.data.AUTOTUNE)\n",
    "        test_data = from_tensor_slices(test_data).cache()\\\n",
    "                                                 .batch(batch_size)\\\n",
    "                                                 .prefetch(tf.data.AUTOTUNE)\n",
    "    return train_data, test_data\n",
    "train_data, test_data = create_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch dataloader / tf dataset\n",
    "\n",
    "The dataloader and TFDataset should return x, y in each iteration with the shape as following:  \n",
    "x's shape is (num_samples, lookback, feature_dim) where lookback and feature_dim should be the same as past_seq_len and input_feature_num.  \n",
    "y's shape is (num_samples, horizon, target_dim), where horizon and target_dim should be the same as future_seq_len and output_feature_num.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below two cells show how to generate the pytorch dataloader or tf dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = create_data(loader=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = create_data(tf_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IswBKf39mexg"
   },
   "source": [
    "# Step2: Training\n",
    "\n",
    "### fit\n",
    "We can train model by calling `fit` with no validation data.  \n",
    "Input the following 3 parameters: `data`, `epochs`, and `batch_size`.  \n",
    "The validation_step in the training loop will be skipped.`fit` has no return value in this way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yc34x9dcmexg",
    "outputId": "bba44274-e6d0-408a-bf9b-32d549fe1ea9"
   },
   "outputs": [],
   "source": [
    "from bigdl.chronos.forecaster.tcn_forecaster import TCNForecaster\n",
    "train_data, val_data, test_data = create_data()\n",
    "forecaster = TCNForecaster(past_seq_len=24,\n",
    "                           future_seq_len=5,\n",
    "                           input_feature_num=1,\n",
    "                           output_feature_num=1,\n",
    "                           kernel_size=4,\n",
    "                           num_channels=[16, 16],\n",
    "                           loss=\"mae\",\n",
    "                           lr=0.01)\n",
    "forecaster.fit(train_data, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZiolCDSmmexj"
   },
   "source": [
    "### validation\n",
    "We can also call fit while having validation data.  \n",
    "Input the following 6 parameters: `data`, `validation_data`, `epochs`, `batch_size`, `validation_mode`, and `earlystop_patience`.  \n",
    "The validation_step in the training loop will be executed. And in this way, you need to input `validation_mode` to select the operation you want.  \n",
    "The `validation_mode` includes the following types: `output`, `earlystop`, and `best_epoch`.  \n",
    "`fit` will return a dict recording the average validation loss of each epoch in this way.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7jWojYNmexk",
    "outputId": "eb4cc68f-4d38-410d-b007-c507da30b97f"
   },
   "outputs": [],
   "source": [
    "forecaster = TCNForecaster(past_seq_len=24,\n",
    "                           future_seq_len=5,\n",
    "                           input_feature_num=1,\n",
    "                           output_feature_num=1,\n",
    "                           kernel_size=4,\n",
    "                           num_channels=[16, 16],\n",
    "                           loss=\"mae\",\n",
    "                           lr=0.01)\n",
    "val_loss = forecaster.fit(train_data, val_data, validation_mode='best_epoch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we choose `best_epoch` for validation mode, forecaster will monitor the val_loss, and load the checkpoint of the epoch with the smallest val_loss after the training.  \n",
    "Time series data might not be that large and overfitting has become a problem for many users. Validation helps to make sure the best ckpt with lowest validation loss is used after the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pxvb3gKcmexk"
   },
   "source": [
    "### automatically acceleration"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "seq_and_func.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
